>Date: 2023-04-18
## 背景

将埋点脚本从 EBS 系统中收集存入到 MongoDB 中的数据，以一定的算法规则指标进行清洗整理，最后能在数据可视化平台上查询展示管理。

首先，我们声明若干个概念：

* MongoDB：源数据库
* MongoDB 存储的数据：源数据
* 清洗整理后存储的数据：目标数据
* 存储目标数据的数据库：目标数据库
## 方案设计

如何保证，我们能在数据可视化平台上看到较为新鲜的数据，数据的流转大致如下：

源数据库 -> 数据清洗服务 -> 目标数据库。

### 源数据库

由后端提供，数据获取方式为

1. 数据清洗服务连接数据库，获取源数据。
2. 后端提供数据获取接口，数据清洗服务请求接口获取源数据。
### 数据服务

最初的技术设计是以 Python + Pandas 为主，提到数据处理，最令人印象深刻的莫过于此组合。但问问自己，我们是否真的需要如此专业的数据处理方案。我们需要的不过是获取源数据，清洗处理，存入目标数据库罢了。所谓的清洗处理，则是以一定的规则转换，过滤，处理数据罢了。目前可预见的数据量级，数据结构、算法的复杂程度尚不至于要求非常专业的数据处理方案。

* 清洗的时间频率：
    * 时间分片：以一定的时间间隔去获取源数据。可以是半小时，也可以是一小时。无论是半小时或一小时，数据量的压力不会太大，且能保证数据的新鲜度。
    * 主动触发：由数据平台用户手动触发，数据清洗服务去获取源数据进行数据清洗。
* 数据清洗逻辑：由开发者自行实现数据处理算法逻辑。
* 主要技术栈
    * 编程语言：node.js
    * 服务框架：Fastify
* 目标数据库：MySQL
### 数据展示平台

数据管理，以各种形式展示目标数据。可能会有埋点事件的定义，编辑。也可以支持算法规则的自定义。

* 主要技术栈
    * 页面编写：Vue.js (为了适应团队大多数人的需要，使用该技术栈)
    * 图表库：ECharts
* 对应的后端服务：数据服务
    * 提供经清洗后的所需数据
    * 提供用户管理 (注册登录)
## 表设计

* 用户表
* 用户会话表
* 用户修改记录表




